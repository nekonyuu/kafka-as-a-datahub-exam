# Projet de fin de cours - Sujet n°2

Depuis les données fournies par Météo France, nous allons calculer des métriques sur plusieurs périodes par 
station (et donc par ville).

## Éléments
### Source de données

La source de données sera un CSV fourni par Meteo France.

Le jeu de données est téléchargeable [ici](https://ams3.digitaloceanspaces.com/nekonyuu42/work/nyuulabs/courses/kafka-data-hub/synop-data-meteo-cleaned.csv.gz?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=DHKQC67GSU2ARRU2Y5EQ%2F20180703%2Fams3%2Fs3%2Faws4_request&X-Amz-Date=20180703T143624Z&X-Amz-Expires=604800&X-Amz-SignedHeaders=host&X-Amz-Signature=ad13597c409557e916860bc07ebb123296452026ba3a369535999d14c638d9c0).

À vous d'inspecter le schéma du CSV, qui est self-explanatory.

### Kafka

Vous pouvez spawn votre propre Kafka si vous avez 
Docker (for Windows / for Mac) !

Une fois Docker et docker-compose (https://docs.docker.com/compose/install/) installé, allez dans le dossier `platform/docker` 
de ce dépôt et faites un `docker-compose up -d`.

Kafka sera exposé sur `localhost:29092` et Zookeeper sur `localhost:32181`.

## Applications à implémenter dans le cadre du projet

Vous devrez faire usage d'une techno de streaming dans ce projet, à choisir dans la dernière partie :).

Chacun de ces traitements devra être fait dans une application séparée 
(architecture de type microservices).

### Part 1 - Chargement des données

Dans un premier temps, il va être nécessaire de lire ces données et de les 
écrire dans un topic Kafka nommé `raw_station_data`. Vous pourrez faire cela de la
manière dont vous le souhaitez. Ces données devront être écrites ligne par ligne de manière
à simuler une arrivée de type streaming.

Vous avez le choix du format d'écriture dans Kafka.

### Part 2 - Aggrégation des données

Une fois écrits dans ce topic, vous devez maintenant regrouper les données par département.

Les données Méteo France contiennent la localisation GPS de la station, 
ce qui vous permettra d'en déduire le départemement.

Le gouvernement fournit une API à l'adresse https://api.gouv.fr/api/api-geo.html.

Usez de tous les moyens à votre disposition pour le faire, la cible devra être écrite dans 
un topic `aggregated_station_data`

### Part 3 - Metrics

Vous devrez ensuite faire des aggrégations par:
  * département
    * température minimale, maximale
    * pression minimale, maximale
  * ville
    * température minimale, maximale
    * pression minimale, maximale

Chacune de ces aggrégations devra aller dans un topic, avec le nommage de votre choix.

### Part 4 - Dataviz

Enfin, développez une application simple exposant ces métriques via une API REST.

Tips How-To: https://blog.codecentric.de/en/2017/03/interactive-queries-in-apache-kafka-streams/ 

En bonus, vous pouvez y ajouter une application front affichant des graphes de
ces aggrégations, en récupérant les infos dans l'API.

Vous ne devez pas utiliser de base de données, les frameworks de streaming 
fournissent de quoi faire tout cela sans base :).

## Technos possibles (non limité)

  * Streams
    * Spark Streaming: https://spark.apache.org/streaming/
    * Kafka Streams: https://kafka.apache.org/documentation/streams/
    * Akka Streams: https://doc.akka.io/docs/akka/2.5/stream/index.html
  * REST APIs
    * Play: https://www.playframework.com/
    * Akka-HTTP: https://doc.akka.io/docs/akka-http/current/
    * Spring Boot: https://spring.io/guides/gs/rest-service/
  * Viz
    * D3.js: https://d3js.org/
    * Leaflet: https://leafletjs.com/
    * Highcharts: https://www.highcharts.com/

